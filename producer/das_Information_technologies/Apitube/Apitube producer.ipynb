{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a663d1-a056-4ba7-8dd1-44cf0606e685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apitube Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e166951-106a-4e7d-b345-1dea7b1a2cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Documentazione del Servizio Apitube\n",
    "- news rest api service:  https://apitube.io/\n",
    "- documentation:          https://docs.apitube.io/guides/user-guide/what-is-apitube\n",
    "- response structure:     https://docs.apitube.io/platform/news-api/response-structure\n",
    "- dashboard api key:      https://dashboard.apitube.io/\n",
    "- cookbook:               https://apitube.io/cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a9a75e2-e663-43d6-9041-e18db58c74c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af5ecf9-0795-4eae-b71b-482b8088e806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### WS Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea054286-4cdb-459e-991f-b9e5e6d9edf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ExternalFunctionRequestHttpMethod\n",
    "\n",
    "# workspace client creation\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d14ea72-dff3-4012-83cb-75759a1feccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Expected Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "49d10cf3-3adf-4b87-9c2e-6b9dfb597560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Schema dettagliato della risposta APITube\n",
    "response_schema = \"\"\"\n",
    "STRUCT<\n",
    "  status: STRING,\n",
    "  page: LONG,\n",
    "  per_page: LONG,\n",
    "  path: STRING,\n",
    "  has_next_pages: BOOLEAN,\n",
    "  next_page: STRING,\n",
    "  has_previous_page: BOOLEAN,\n",
    "  previous_page: STRING,\n",
    "  export: STRUCT<\n",
    "    json: STRING,\n",
    "    xlsx: STRING,\n",
    "    csv: STRING,\n",
    "    tsv: STRING,\n",
    "    xml: STRING\n",
    "  >,\n",
    "  request_id: STRING,\n",
    "  results: ARRAY<STRUCT<\n",
    "    id: LONG,\n",
    "    href: STRING,\n",
    "    published_at: STRING,\n",
    "    title: STRING,\n",
    "    description: STRING,\n",
    "    body: STRING,\n",
    "    language: STRING,\n",
    "\n",
    "    author: STRUCT<\n",
    "      id: LONG,\n",
    "      name: STRING\n",
    "    >,\n",
    "\n",
    "    image: STRING,\n",
    "\n",
    "    categories: ARRAY<STRUCT<\n",
    "      id: STRING,\n",
    "      name: STRING,\n",
    "      score: DOUBLE,\n",
    "      taxonomy: STRING,\n",
    "      links: STRUCT<\n",
    "        self: STRING\n",
    "      >\n",
    "    >>,\n",
    "\n",
    "  topics: ARRAY<STRUCT<\n",
    "    id: STRING,\n",
    "    name: STRING,\n",
    "    score: DOUBLE,\n",
    "    taxonomy: STRING,\n",
    "    links: STRUCT<\n",
    "      self: STRING\n",
    "    >\n",
    "  >>,\n",
    "\n",
    "    industries: ARRAY<STRUCT<\n",
    "      id: LONG,\n",
    "      name: STRING,\n",
    "      links: STRUCT<\n",
    "        self: STRING\n",
    "      >\n",
    "    >>,\n",
    "\n",
    "    entities: ARRAY<STRUCT<\n",
    "      id: LONG,\n",
    "      name: STRING,\n",
    "      links: STRUCT<\n",
    "        self: STRING,\n",
    "        wikipedia: STRING,\n",
    "        wikidata: STRING\n",
    "      >,\n",
    "      types: ARRAY<STRING>,\n",
    "      language: STRING,\n",
    "      frequency: LONG,\n",
    "      title: STRUCT<\n",
    "        pos: ARRAY<STRUCT<\n",
    "          start: LONG,\n",
    "          end: LONG\n",
    "        >>\n",
    "      >,\n",
    "      body: STRUCT<\n",
    "        pos: ARRAY<STRUCT<\n",
    "          start: LONG,\n",
    "          end: LONG\n",
    "        >>\n",
    "      >\n",
    "    >>,\n",
    "\n",
    "    source: STRUCT<\n",
    "      id: LONG,\n",
    "      domain: STRING,\n",
    "      home_page_url: STRING,\n",
    "      type: STRING,\n",
    "      bias: STRING,\n",
    "      rankings: STRUCT<\n",
    "        opr: DOUBLE\n",
    "      >,\n",
    "      location: STRUCT<\n",
    "        country_name: STRING,\n",
    "        country_code: STRING\n",
    "      >,\n",
    "      favicon: STRING\n",
    "    >,\n",
    "\n",
    "    sentiment: STRUCT<\n",
    "      overall: STRUCT<\n",
    "        score: DOUBLE,\n",
    "        polarity: STRING\n",
    "      >,\n",
    "      title: STRUCT<\n",
    "        score: DOUBLE,\n",
    "        polarity: STRING\n",
    "      >,\n",
    "      body: STRUCT<\n",
    "        score: DOUBLE,\n",
    "        polarity: STRING\n",
    "      >\n",
    "    >,\n",
    "\n",
    "    summary: ARRAY<STRUCT<\n",
    "      sentence: STRING,\n",
    "      sentiment: STRUCT<\n",
    "        score: DOUBLE,\n",
    "        polarity: STRING\n",
    "      >\n",
    "    >>,\n",
    "\n",
    "    keywords: ARRAY<STRING>,\n",
    "    links: ARRAY<STRING>,\n",
    "\n",
    "    media: ARRAY<STRUCT<\n",
    "      url: STRING,\n",
    "      type: STRING,\n",
    "      format: STRING\n",
    "    >>,\n",
    "\n",
    "    story: STRUCT<\n",
    "      id: LONG,\n",
    "      uri: STRING\n",
    "    >,\n",
    "\n",
    "    is_duplicate: BOOLEAN,\n",
    "    is_paywall: BOOLEAN,\n",
    "    is_breaking: BOOLEAN,\n",
    "\n",
    "    read_time: LONG,\n",
    "    sentences_count: LONG,\n",
    "    paragraphs_count: LONG,\n",
    "    words_count: LONG,\n",
    "    characters_count: LONG\n",
    "  >>\n",
    ">\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9a02da-4246-45aa-b97c-0831584f2c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Kafka Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9f67a3-b23c-4a19-a63b-e5d59709c037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def kafka_setup():\n",
    "\n",
    "    # Event Hubs configuration\n",
    "    EH_NAMESPACE                    = \"aeh-ag83-cm-eventhub-001\" # spark.conf.get(\"iot.ingestion.eh.namespace\")\n",
    "    EH_NAME                         = \"eventhub-001\" # spark.conf.get(\"iot.ingestion.eh.name\")\n",
    "    EH_CONN_SHARED_ACCESS_KEY_NAME  = \"aar-ag83-cm-eventhub-001\" # spark.conf.get(\"iot.ingestion.eh.accessKeyName\")\n",
    "\n",
    "    # Event Hubs configuration - key\n",
    "    SECRET_SCOPE                    = \"scp-ws-pipelines\" # spark.conf.get(\"io.ingestion.eh.secretsScopeName\")\n",
    "    SECRET_NAME                     = \"scr-eventhub-key\" # spark.conf.get(\"io.ingestion.eh.secretName\")\n",
    "    EH_CONN_SHARED_ACCESS_KEY_VALUE = dbutils.secrets.get(scope = f\"{SECRET_SCOPE}\", key = f\"{SECRET_NAME}\")\n",
    "\n",
    "    # Event Hubs configuration - connectionString\n",
    "    EH_CONN_STR                     = f\"Endpoint=sb://{EH_NAMESPACE}.servicebus.windows.net/;SharedAccessKeyName={EH_CONN_SHARED_ACCESS_KEY_NAME};SharedAccessKey={EH_CONN_SHARED_ACCESS_KEY_VALUE}\"\n",
    "    # Kafka Consumer configuration\n",
    "\n",
    "    KAFKA_OPTIONS = {\n",
    "      \"kafka.bootstrap.servers\"  : f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "      \"topic\"                    : EH_NAME,\n",
    "      \"kafka.sasl.mechanism\"     : \"PLAIN\",\n",
    "      \"kafka.security.protocol\"  : \"SASL_SSL\",\n",
    "      \"kafka.sasl.jaas.config\"   : f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{EH_CONN_STR}\\\";\",\n",
    "      \"kafka.request.timeout.ms\" : 60000,\n",
    "      \"kafka.session.timeout.ms\" : 30000,\n",
    "      \"maxOffsetsPerTrigger\"     : 50000,\n",
    "      \"failOnDataLoss\"           : 'false',\n",
    "      \"startingOffsets\"          : 'latest'\n",
    "    }\n",
    "    return KAFKA_OPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55a40388-301a-4e3f-afab-d6d66a3a4870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apitube getData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0235ed3f-9b6b-45cf-bbb1-93c7d8073b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Get daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457cb42d-e46f-4080-b3ad-5c4078dcea95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, explode, col, current_timestamp\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def getdata_daily(process_date: str):\n",
    "    response = w.serving_endpoints.http_request(\n",
    "        conn=\"api-apitube-http\",   \n",
    "        method=ExternalFunctionRequestHttpMethod.GET,\n",
    "        path=\"\",\n",
    "        params={\n",
    "            # \"title\": \"Databricks,Iveco,Luxottica,Essilor,Teamsystem,Prysmian,Milan,Rome,Roma\",\n",
    "            # \"language.code\": \"it\",\n",
    "            # se vuoi altri filtri li aggiungi qui, es:\n",
    "            # \"location.name\": \"Milan,Rome\",\n",
    "            # \"ignore.location.name\": \"New York\",\n",
    "            \"published_at\": process_date,\n",
    "        },\n",
    "        headers={}\n",
    "    )\n",
    "\n",
    "    raw_json = response.text  # stringa JSON APITube\n",
    "\n",
    "    # 2. Crea un DataFrame con la risposta grezza\n",
    "    df_raw = spark.createDataFrame([Row(raw_response=raw_json)])\n",
    "\n",
    "    # aggiungi colonna timestamp corrente\n",
    "    df_raw = df_raw.withColumn(\"getdata_timestamp\", current_timestamp())\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "237ac5bb-9951-44e4-9d15-1937e157f067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Get weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57fed002-3599-4832-b798-af5ba9583057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, explode, col, current_timestamp\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def getdata_weekly(process_date: str):\n",
    "    # Calcola l'intervallo di 7 giorni\n",
    "    process_week_start = process_date\n",
    "    process_week_end = (datetime.strptime(process_date, \"%Y-%m-%d\") + timedelta(days=6)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(f\"Sto considerando la settimana con data inizio: {process_week_start} e data fine: {process_week_end}\")\n",
    "\n",
    "    response = w.serving_endpoints.http_request(\n",
    "        conn=\"api-apitube-http\",   \n",
    "        method=ExternalFunctionRequestHttpMethod.GET,\n",
    "        path=\"\",\n",
    "        params={\n",
    "            # \"title\": \"Databricks,Iveco,Luxottica,Essilor,EssilorLuxottica,Teamsystem,Prysmian,Milan,Rome,Roma\",\n",
    "            \"published_at.start\": process_week_start,\n",
    "            \"published_at.end\": process_week_end,\n",
    "        },\n",
    "        headers={}\n",
    "    )\n",
    "\n",
    "    raw_json = response.text  # stringa JSON APITube\n",
    "\n",
    "    # 2. Crea un DataFrame con la risposta grezza\n",
    "    df_raw = spark.createDataFrame([Row(raw_response=raw_json)])\n",
    "\n",
    "    # aggiungi colonna timestamp corrente\n",
    "    df_raw = df_raw.withColumn(\"getdata_timestamp\", current_timestamp())\n",
    "\n",
    "    display(df_raw)\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b85d2d12-3d17-4b1e-a783-998022692715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apitube Enrich Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f606c4-ea3b-4e4f-9ed6-7c07785293f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def enrichData(df_raw):\n",
    "    # 4. Parsing del JSON e esplosione dell'array results\n",
    "    parsed = df_raw.select(\n",
    "        from_json(col(\"raw_response\"), response_schema).alias(\"data\"),\n",
    "        col(\"getdata_timestamp\")\n",
    "    )\n",
    "\n",
    "    articles = (\n",
    "        parsed\n",
    "        .select(explode(col(\"data.results\")).alias(\"article\"), col(\"getdata_timestamp\"))\n",
    "        .select(\"article.*\", \"getdata_timestamp\")\n",
    "        .withColumn(\"enrich_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a450f9da-fa63-4a0f-ab0b-990063bd3f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apitube Load Data to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c752cec-d8d4-47f3-8acd-d87f74f4425b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "def loadData(articles):\n",
    "    # scegli catalogo e schema esistenti\n",
    "    catalog_name = \"das_information_technologies\"         \n",
    "    schema_name  = \"apitube\"       \n",
    "    table_name   = \"apitube_articles\"\n",
    "\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "    articles = articles.withColumn(\"loaddata_timestamp\", current_timestamp())\n",
    "\n",
    "    (\n",
    "        articles\n",
    "        .write\n",
    "        .mode(\"append\")   # o \"append\" se vuoi accumulare\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(full_table_name)\n",
    "    )\n",
    "\n",
    "    num_records = articles.count()\n",
    "    print(f\"Tabella scritta: {full_table_name} - Numero record: {num_records}\")\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1cd603c-eb80-4709-a376-fbcfb90c7545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kafka send records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69583d4a-f8a0-461b-a476-cb3accf30c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import col, to_json, struct, array, struct as pyspark_struct, lit\n",
    "\n",
    "def send_kafka(df, KAFKA_OPTIONS):\n",
    "    articles_kafka = df.withColumn(\n",
    "        \"value\",\n",
    "        to_json(struct([col for col in df.columns]), {\"pretty\": \"true\"})\n",
    "    ).withColumn(\n",
    "        \"headers\",\n",
    "        array(\n",
    "            pyspark_struct(\n",
    "                lit(\"content-type\").alias(\"key\"),\n",
    "                lit(\"application/json\").cast(\"binary\").alias(\"value\")\n",
    "            )\n",
    "        )\n",
    "    ).selectExpr(\"CAST(value AS STRING) as value\", \"headers\")\n",
    "\n",
    "    articles_kafka.write.format(\"kafka\").options(\n",
    "        **KAFKA_OPTIONS\n",
    "    ).option(\"includeHeaders\", \"true\").save()\n",
    "\n",
    "    num_records = articles_kafka.count()\n",
    "    print(f\"Spediti a Kafka #record: {num_records}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9768fa2d-f67c-4220-87bc-b16a72f46bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Wait Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cea7f5f-4213-40b2-879e-c48d98907520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def waitNextData(min_frequency):\n",
    "    # Attendi min_frequency minuti\n",
    "    sleep_seconds = min_frequency * 40\n",
    "    print(f\"Attendo {min_frequency} minuti prima della prossima data...\")\n",
    "    time.sleep(sleep_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "111c1ea3-5600-4764-b460-30461169c611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test Apitube ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ebe0152-7c1b-4150-9ead-0c92a1fa5b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Daily ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5c1a48-4542-4885-9011-597f5bbf9c26",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766332106513}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_daily(process_date):\n",
    "    \n",
    "    df_raw = getdata_daily(process_date)\n",
    "    display(df_raw)\n",
    "\n",
    "    articles = enrichData(df_raw)\n",
    "    display(articles)\n",
    "\n",
    "    loadData(articles)\n",
    "\n",
    "# esecuzione test\n",
    "test_daily(\"2025-12-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117960f6-94e3-40e3-baef-7b070fae8360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Weekly ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32580221-c767-45d1-b4f7-a609e7fc6024",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766475968493}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_weekly(process_date):\n",
    "    \n",
    "    df_raw = getdata_weekly(process_date)\n",
    "    display(df_raw)\n",
    "\n",
    "    articles = enrichData(df_raw)\n",
    "    display(articles)\n",
    "\n",
    "    loadData(articles)\n",
    "\n",
    "# esecuzione test\n",
    "test_weekly(\"2025-12-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15f0a5a0-2980-485a-8415-655acafac4eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test Apitube ETL + Kafka sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa33957-b222-48e6-8ec2-a0c259ee76b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "k_opt = kafka_setup()\n",
    "\n",
    "df_raw = getdata_daily('2025-12-16')\n",
    "display(df_raw)\n",
    "\n",
    "enriched_articles = enrichData(df_raw)    \n",
    "loaded_articles = loadData(enriched_articles)    \n",
    "display(loaded_articles)\n",
    "\n",
    "send_kafka(loaded_articles, k_opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c34d748-32bd-4698-a573-da40d34f78c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b988c3-e2ee-4726-95e3-9d5f82758b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# kafka setup\n",
    "k_opt = kafka_setup()\n",
    "\n",
    "# ============================\n",
    "# 1. Parametri dal Job\n",
    "# ============================\n",
    "\n",
    "# Data iniziale (yyyy-MM-dd), es: \"2015-01-01\"\n",
    "dbutils.widgets.text(\"start_date\", \"2024-01-01\", \"Data iniziale (yyyy-MM-dd)\")\n",
    "start_date_str = dbutils.widgets.get(\"start_date\")\n",
    "start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "# Frequenza in minuti tra una iterazione e la successiva\n",
    "dbutils.widgets.text(\"min_frequency\", \"1\", \"Frequenza minuti\")\n",
    "min_freq_str = dbutils.widgets.get(\"min_frequency\")\n",
    "min_frequency = int(min_freq_str)\n",
    "\n",
    "print(f\"Data iniziale: {start_date}\")\n",
    "print(f\"Frequenza: {min_frequency} minuti\")\n",
    "\n",
    "# ============================\n",
    "# 2. Ciclo infinito sulle date\n",
    "# ============================\n",
    "\n",
    "current_date = start_date\n",
    "\n",
    "while True:\n",
    "    process_date = current_date\n",
    "    process_date_str = process_date.isoformat()\n",
    "\n",
    "    print(f\"\\n=== Elaboro data = {process_date_str} ===\")\n",
    "\n",
    "    df_raw = getdata_weekly(process_date_str)\n",
    "    enriched_articles = enrichData(df_raw)    \n",
    "    loaded_articles = loadData(enriched_articles)    \n",
    "    send_kafka(loaded_articles, k_opt)\n",
    "    \n",
    "    print(f\"Data elaborata: {process_date_str}\")\n",
    "\n",
    "    # Incrementa la data di 1 giorno per il prossimo giro\n",
    "    current_date = current_date + timedelta(days=6)\n",
    "\n",
    "    waitNextData(min_frequency)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apitube producer",
   "widgets": {
    "min_frequency": {
     "currentValue": "1",
     "nuid": "598c5a21-0438-4d50-8ad1-93dc90b4427f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": "Frequenza minuti",
      "name": "min_frequency",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1",
      "label": "Frequenza minuti",
      "name": "min_frequency",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "2025-01-01",
     "nuid": "a9779e04-4d43-4e84-9077-dec79b0232e1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2015-01-01",
      "label": "Data iniziale (yyyy-MM-dd)",
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2015-01-01",
      "label": "Data iniziale (yyyy-MM-dd)",
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
